{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpuem+1X/hwOGOi4G4uHZ5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robitussin/CMSCSNLP/blob/main/emotiondetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLR5RabAsqyB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03aab60d-6a82-4b67-b7b8-ec1e6123df0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[?25l\r\u001b[K     |‚ñà‚ñâ                              | 10 kB 18.4 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñä                            | 20 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                          | 30 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 40 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 51 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 61 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 71 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 81 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 92 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 102 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 112 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 122 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 133 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 143 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 153 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 163 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 174 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 175 kB 5.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=c2f3ed62da86703bd8e11e70a6a5cbcea44345c05756246522ab73e87d65d065\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install emoji"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import all dependencies and a list of tagalog stop words"
      ],
      "metadata": {
        "id": "BlWEXFkxz3YA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re, emoji, os, random\n",
        "import nltk as nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Tagalog stop word list \n",
        "stop_words = ['akin','aking','ako','alin','am','amin','aming','ang','ano','anumang','apat','at','atin','ating','ay','bababa','bago','bakit','bawat','bilang','dahil',\n",
        "             'dalawa','dapat','din','dito','doon','gagawin','gayunman','ginagawa','ginawa','ginawang','gumawa','gusto','habang','hanggang','hindi','huwag','iba','ibaba',\n",
        "             'ibabaw','ibig','ikaw','ilagay','ilalim','ilan','inyong','isa','isang','itaas','ito','iyo','iyon','iyong','ka','kahit','kailangan','kailanman','kami','kanila',\n",
        "             'kanilang','kanino','kanya','kanyang','kapag','kapwa','karamihan','katiyakan','katulad','kaya','kaysa','ko','kong','kulang','kumuha','kung','laban','lahat','lamang',\n",
        "             'likod','lima','maaari','maaaring','maging','mahusay','makita','marami','marapat','masyado','may','mayroon','mga','minsan','mismo','mula','muli','na','nabanggit','naging',\n",
        "             'nagkaroon','nais','nakita','namin','napaka','narito','nasaan','ng','ngayon','ni','nila','nilang','nito','niya','niyang','noon','o','pa','paano','pababa','paggawa','pagitan',\n",
        "             'pagkakaroon','pagkatapos','palabas','pamamagitan','panahon','pangalawa','para','paraan','pareho','pataas','pero','pumunta','pumupunta','sa','saan','sabi','sabihin','sarili','sila','sino','siya','tatlo','tayo','tulad','tungkol','una','walang']"
      ],
      "metadata": {
        "id": "_4XP6MQztUNr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b0359d0-0ade-4cc2-d572-67ae7260a1a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data Set from github repository"
      ],
      "metadata": {
        "id": "lGlQ4oZy1P04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://github.com/robitussin/emotiondetection/blob/main/datasets/mergedset.xlsx?raw=true'\n",
        "dataset = pd.read_excel(url)"
      ],
      "metadata": {
        "id": "F20GuRfjtXBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utility functions\n",
        "\n",
        "1.   A function for text cleaning to remove whitespaces and non-alphabetical characters\n",
        "2.   A function to insert punctuation marks for data augmentation\n",
        "3.   A function to count the number of word per comment\n",
        "\n"
      ],
      "metadata": {
        "id": "42_HHQCX1zPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove any other characters other than alphabetical characters\n",
        "# Remove white spaces\n",
        "def cleaner(text):\n",
        "    text = re.sub('[^a-zA-Z]', '', str(text))\n",
        "    text = re.sub(' +', ' ', str(text))\n",
        "\n",
        "    cleaned_text = text.strip()\n",
        "    return cleaned_text\n",
        "\n",
        "def word_count_per_doc(text):\n",
        "\ttokenized = word_tokenize(cleaner(text))\n",
        "\treturn len(tokenized)\n",
        " \n",
        "PUNCTUATIONS = ['.', ',', '!', '?', ';', ':']\n",
        "PUNC_RATIO = 0.3\n",
        "\n",
        "# Data Augmentation Technique\n",
        "def insert_punctuation_marks(sentence, punc_ratio=PUNC_RATIO):\n",
        "\twords = sentence.split(' ')\n",
        "\tnew_line = []\n",
        "\tq = random.randint(1, int(punc_ratio * len(words) + 1))\n",
        "\tqs = random.sample(range(0, len(words)), q)\n",
        "\n",
        "\tfor j, word in enumerate(words):\n",
        "\t\tif j in qs:\n",
        "\t\t\tnew_line.append(PUNCTUATIONS[random.randint(0, len(PUNCTUATIONS)-1)])\n",
        "\t\t\tnew_line.append(word)\n",
        "\t\telse:\n",
        "\t\t\tnew_line.append(word)\n",
        "\tnew_line = ' '.join(new_line)\n",
        "\treturn new_line"
      ],
      "metadata": {
        "id": "A1_vW28SvhqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning Process\n",
        "\n",
        "\n",
        "1. Duplicates were removed\n",
        "2. Rows with NULL values were removed\n",
        "3. All comments were converted to lower text\n",
        "4. Relabeled emotions\n",
        "5. Renamed column names\n"
      ],
      "metadata": {
        "id": "tjhRiBJC1Urz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicates\n",
        "dataset = dataset.drop_duplicates(subset=['COMMENTS'])\n",
        "\n",
        "# Remove rows with NULL value\n",
        "dataset = dataset.dropna().reset_index(drop=True)\n",
        "\n",
        "# Convert all text to lower case\n",
        "dataset = dataset.apply(lambda x: x.astype(str).str.lower())\n",
        "\n",
        "# Re label misspelled and similar labels\n",
        "dataset[\"MAJORITY\"].replace({\"sad\": \"sadness\", \"0\": \"none\", \"digust\": \"disgust\"}, inplace=True)\n",
        "\n",
        "dataset = dataset.rename(columns={\"COMMENTS\": \"comments\", \"MAJORITY\": \"label\"})\n",
        "\n",
        "print(dataset.info())"
      ],
      "metadata": {
        "id": "0yc_VLDfu4GV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd6fc8de-1e4b-4693-b222-bef9dd6b8a59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 17771 entries, 0 to 17770\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   comments  17771 non-null  object\n",
            " 1   label     17771 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 277.8+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get total number of rows for each label/emotion"
      ],
      "metadata": {
        "id": "kAJhqQmh1nTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_none, class_sadness, class_anger, class_joy, class_fear, class_surprise, class_disgust = dataset.label.value_counts()"
      ],
      "metadata": {
        "id": "K0emxGRAvN2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_none = dataset.loc[dataset['label'] == \"none\"]\n",
        "dataset_sadness = dataset.loc[dataset['label'] == \"sadness\"]\n",
        "dataset_anger = dataset.loc[dataset['label'] == \"anger\"]\n",
        "dataset_joy = dataset.loc[dataset['label'] == \"joy\"]\n",
        "dataset_fear = dataset.loc[dataset['label'] == \"fear\"]\n",
        "dataset_surprise = dataset.loc[dataset['label'] == \"surprise\"]\n",
        "dataset_disgust = dataset.loc[dataset['label'] == \"disgust\"]"
      ],
      "metadata": {
        "id": "42AZv3zcvPEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the number of records for each class/emotion. Choose to either under sample (downsize) or over sample (augment) data."
      ],
      "metadata": {
        "id": "tFaQsFW54DUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset['label'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LolWreoP3zAk",
        "outputId": "4fa9582a-0386-4971-9e5b-ac68295c5540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "none        4974\n",
            "sadness     4749\n",
            "anger       4168\n",
            "joy         3344\n",
            "fear         214\n",
            "surprise     194\n",
            "disgust      128\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Important Note: only choose to under sample or over sample when doing an experiment. Do not run both undersample and oversample cells"
      ],
      "metadata": {
        "id": "6bOYUuLr0Uxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "UnderSample\n",
        "- Get the label/emotion with the least number of rows in the data set. Downsize all other classes/emotions to make them all have an equal number of rows."
      ],
      "metadata": {
        "id": "7SstTEoo1JBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampleCount = class_disgust;\n",
        "\n",
        "dataset_none_under = dataset_none.sample(sampleCount)\n",
        "dataset_sadness_under = dataset_sadness.sample(sampleCount)\n",
        "dataset_anger_under = dataset_anger.sample(sampleCount)\n",
        "dataset_joy_under = dataset_joy.sample(sampleCount)\n",
        "dataset_fear_under = dataset_fear.sample(sampleCount)\n",
        "dataset_surprise_under = dataset_surprise.sample(sampleCount)\n",
        "\n",
        "balanced_dataset = pd.concat([dataset_disgust, dataset_none_under, dataset_sadness_under, dataset_anger_under, dataset_joy_under, dataset_fear_under, dataset_surprise_under], ignore_index=True)\n",
        "\n",
        "print(balanced_dataset.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmOtZfZQ0A95",
        "outputId": "0efe91c7-0253-4b3d-fee6-1ae011b6f90f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 896 entries, 0 to 895\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   comments  896 non-null    object\n",
            " 1   label     896 non-null    object\n",
            "dtypes: object(2)\n",
            "memory usage: 14.1+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OverSample\n",
        "- Get the label/emotion with the largest number of rows in the dataset. Augment all other classes/emotions to make them all have an equal amount of rows. "
      ],
      "metadata": {
        "id": "Zu_vBncT1G3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sampleCount = class_none;\n",
        "\n",
        "# Value hardcoded to prevent exceeding RAM usage\n",
        "sampleCount = 3000; \n",
        "\n",
        "dataset_disgust_over = dataset_disgust.sample(sampleCount, replace=True)\n",
        "dataset_sadness_over  = dataset_sadness.sample(sampleCount, replace=True)\n",
        "dataset_anger_over  = dataset_anger.sample(sampleCount, replace=True)\n",
        "dataset_joy_over  = dataset_joy.sample(sampleCount, replace=True)\n",
        "dataset_fear_over  = dataset_fear.sample(sampleCount, replace=True)\n",
        "dataset_surprise_over  = dataset_surprise.sample(sampleCount, replace=True)\n",
        "\n",
        "balanced_dataset = pd.concat([dataset_none, dataset_disgust_over, dataset_sadness_over, dataset_anger_over, dataset_joy_over, dataset_fear_over, dataset_surprise_over], ignore_index=True)\n",
        "\n",
        "print(balanced_dataset.info())"
      ],
      "metadata": {
        "id": "RSN7dzbBvQfi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b85a6600-21ae-40a2-ae4d-10f5f0d80925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 22974 entries, 0 to 22973\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   comments  22974 non-null  object\n",
            " 1   label     22974 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 359.1+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shuffle data set and insert punctuation marks for data augmentation"
      ],
      "metadata": {
        "id": "0sHxpy680vKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_dataset = shuffle(balanced_dataset)\n",
        "dataset_X = balanced_dataset[['comments']]\n",
        "dataset_X = dataset_X['comments'].apply(insert_punctuation_marks).to_frame()\n",
        "dataset_y = balanced_dataset['label']"
      ],
      "metadata": {
        "id": "qdyH5RzPvTIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split data set to train and test"
      ],
      "metadata": {
        "id": "T15-fovM03GS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(dataset_X, dataset_y, test_size=0.20, random_state=1)"
      ],
      "metadata": {
        "id": "XVIJ8lcKvcd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split data set to test and validation"
      ],
      "metadata": {
        "id": "ULi_pJAI05kR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.50, random_state=1)"
      ],
      "metadata": {
        "id": "jTPoUVSzvdNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.reset_index(drop=True, inplace=True)\n",
        "X_test.reset_index(drop=True, inplace=True)\n",
        "X_val.reset_index(drop=True, inplace=True)\n",
        "\n",
        "y_train.reset_index(drop=True, inplace=True)\n",
        "y_test.reset_index(drop=True, inplace=True)\n",
        "y_val.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "NLx2yPqdve-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Traditional Features\n",
        "\n",
        "1. Feature to check angry emojis\n",
        "2. Feature to check sad emojis\n",
        "3. Feature to check joyful emojis\n",
        "4. Feature to check emojis showing disgust\n",
        "5. Feature to check emojis showing fear\n",
        "6. Feature to check emojis showing surprise\n",
        "7. Word Frequency feature\n",
        "8. Vowel count feature\n",
        "9. Consonant count feature"
      ],
      "metadata": {
        "id": "5lNMfzRH11Hi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_angry_emojis(comment):\n",
        "   emojis = ''.join(character for character in comment if character in emoji.UNICODE_EMOJI['en'])\n",
        "\n",
        "   line = [\"üñï\", \"üí©\", \"üò§\", \"üò°\", \"üò†\", \"ü§¨\"]\n",
        "   for character in emojis:\n",
        "\t   if character in line:\n",
        "\t\t   return 1\n",
        "   return 0\n",
        "\n",
        "def check_sad_emojis(comment):\n",
        "   emojis = ''.join(character for character in comment if character in emoji.UNICODE_EMOJI['en'])\n",
        "\n",
        "   line = [\"üòü\", \"üôÅ\t\", \"‚òπ\", \"üò°\", \"üò∞\", \"üò•\", \"üò¢\", \"üò≠\", \"üòì\", \"üíî\"]\n",
        "   for character in emojis:\n",
        "\t   if character in line:\n",
        "\t\t   return 1\n",
        "   return 0\n",
        "\n",
        "def check_joy_emojis(comment):\n",
        "   emojis = ''.join(character for character in comment if character in emoji.UNICODE_EMOJI['en'])\n",
        "\n",
        "   line = [\"üòÄ\", \"üòÉ\", \"üòÑ\", \"üòÅ\", \"üòÜ\", \"ü§£\", \"üòÇ\", \"üôÇ\", \"üòä\", \"üòá\", \"ü•∞\", \"üòç\", \"ü§ó\", \"‚ù§\"]\n",
        "   for character in emojis:\n",
        "\t   if character in line:\n",
        "\t\t   return 1\n",
        "   return 0\n",
        "\n",
        "def check_disgust_emojis(comment):\n",
        "   emojis = ''.join(character for character in comment if character in emoji.UNICODE_EMOJI['en'])\n",
        "\n",
        "   line = [\"ü§¢\", \"ü§Æ\"]\n",
        "   for character in emojis:\n",
        "\t   if character in line:\n",
        "\t\t   return 1\n",
        "   return 0\n",
        "\n",
        "def check_fear_emojis(comment):\n",
        "   emojis = ''.join(character for character in comment if character in emoji.UNICODE_EMOJI['en'])\n",
        "\n",
        "   line = [\"üò®\", \"üò∞\", \"üò±\"]\n",
        "   for character in emojis:\n",
        "\t   if character in line:\n",
        "\t\t   return 1\n",
        "   return 0\n",
        "\n",
        "def check_surprise_emojis(comment):\n",
        "   emojis = ''.join(character for character in comment if character in emoji.UNICODE_EMOJI['en'])\n",
        "\n",
        "   line = [\"üòÆ\", \"üòØ\", \"üò≤\", \"üò≥\"]\n",
        "   for character in emojis:\n",
        "\t   if character in line:\n",
        "\t\t   return 1\n",
        "   return 0\n",
        "\n",
        "def wordFrequency(sentences):\n",
        "\tsentences = list(sentences)\n",
        "\tsentences = [word_tokenize(sentence) for sentence in sentences]\n",
        "\tfor i in range(len(sentences)):\n",
        "\t\t\tsentences[i] = [word for word in sentences[i] if word not in stop_words]\n",
        "\treturn sentences\n",
        "\n",
        "def vowel_count(text):\n",
        "\tsyllable_counts = 0\n",
        "\tfor char in text:\n",
        "\t\tif char == 'a' or char == 'e' or char == 'i' or char == 'o' or char == 'u' or char == 'A' or char == 'E' or char == 'I' or char == 'O' or char == 'U':\n",
        "\t\t\tsyllable_counts += 1\n",
        "\treturn syllable_counts\n",
        "\n",
        "\n",
        "def consonant_count(article):\n",
        "    article = article.lower()\n",
        "    total_consonant = 0\n",
        "\n",
        "    for i in article:\n",
        "        if i == 'b' or i == 'c' or i == 'd' or i == 'f' or i == 'g' \\\n",
        "                or i == 'h' or i == 'j' or i == 'k' or i == 'l' \\\n",
        "                or i == 'm' or i == 'n' or i == 'p' or i == 'q' \\\n",
        "                or i == 'r' or i == 's' or i == 't' or i == 'v' \\\n",
        "                or i == 'w' or i == 'x' or i == 'y' or i == 'z':\n",
        "            total_consonant = total_consonant + 1;\n",
        "\n",
        "    return total_consonant"
      ],
      "metadata": {
        "id": "EsZ32qYKvqNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ortography Features\n",
        "\n",
        "1. Consonant cluster feature\n",
        "\n"
      ],
      "metadata": {
        "id": "Cub0iwhP13eZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_consonant_cluster(text):\n",
        "    cleaned = cleaner(text)\n",
        "    word_count = word_count_per_doc(text)\n",
        "\n",
        "    pattern = \"([bcdfghjklmnpqrstvwxyz]{1}[bcdfghjklmnpqrstvwxyz]{1}[bcdfghjklmnpqrstvwxyz]*)\"\n",
        "    matches = len(re.findall(pattern, cleaned))\n",
        "\n",
        "    result = 0;\n",
        "    if word_count > 0:\n",
        "        matches / word_count\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "iP8iQMOZvswo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Morphological Features\n",
        "\n",
        "1. Auxiliary verb ratio\n",
        "2. Lexical Density feature\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0mlhwjxk17mw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def aux_verb_ratio(text):\n",
        "    splitted = re.split('[?.]+', text)\n",
        "    splitted = [i for i in splitted if i]   #removes empty strings in list\n",
        "\n",
        "    word_count = word_count_per_doc(text)\n",
        "\n",
        "    verb_counter = 0\n",
        "    aux_verbs = 0\n",
        "    for i in splitted:\n",
        "        i = i.strip()\n",
        "        tagged_text = pos_tagger.tag(word_tokenize(i))\n",
        "        for x in tagged_text:\n",
        "            if '|' not in x[0]:\n",
        "                pos = x[1].split('|')[1]\n",
        "                #print(pos)\n",
        "                if pos[:2] == 'VB':\n",
        "                    verb_counter += 1\n",
        "                if pos == 'VBS':\n",
        "                    aux_verbs += 1\n",
        "\n",
        "    if word_count == 0:\n",
        "        return 0\n",
        "\n",
        "    return (aux_verbs/word_count)\n",
        "\n",
        "def lexical_density(text):\n",
        "    splitted = re.split('[?.]+', text)\n",
        "    splitted = [i for i in splitted if i]   #removes empty strings in list\n",
        "\n",
        "    lexical_item_counter = 0\n",
        "    for i in splitted:\n",
        "        i = i.strip()\n",
        "        tagged_text = pos_tagger.tag(word_tokenize(i))\n",
        "        for x in tagged_text:\n",
        "            if '|' not in x[0]:\n",
        "                pos = x[1].split('|')[1]\n",
        "                if pos[:2] == 'VB' or pos[:2] == 'NN' or pos[:2] == 'JJ' or pos[:2] == 'RB':\n",
        "                    lexical_item_counter += 1\n",
        "\n",
        "    word_count = word_count_per_doc(text)\n",
        "    print(\"Word Count:\",word_count)\n",
        "    if word_count == 0:\n",
        "        return 0\n",
        "    return (lexical_item_counter/word_count_per_doc(text))"
      ],
      "metadata": {
        "id": "2AIaIFVovwYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Extraction of Training Set. Get 10 features\n",
        "\n"
      ],
      "metadata": {
        "id": "r-Q4cdnv2CXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit_transform(X_train['comments'])\n",
        "\n",
        "# Feature 1 - Word Frequency\n",
        "X_f1 = X_train['comments'].apply(wordFrequency)\n",
        "X_f1 = vectorizer.transform(X_train['comments'])\n",
        "X_f1 = pd.DataFrame(X_f1.toarray())\n",
        "\n",
        "# Feature 2 - Emojis(Sad)\n",
        "X_f2 = X_train['comments'].apply(check_sad_emojis)\n",
        "\n",
        "# Feature 3 - Emojis(Angry)\n",
        "X_f3 = X_train['comments'].apply(check_angry_emojis)\n",
        "\n",
        "# Feature 4 - Emojis(Joy)\n",
        "X_f4 = X_train['comments'].apply(check_joy_emojis)\n",
        "\n",
        "# Feature 5 - Emojis(Disgust)\n",
        "X_f5 = X_train['comments'].apply(check_disgust_emojis)\n",
        "\n",
        "# Feature 6 - Emojis(Fear)\n",
        "X_f6 = X_train['comments'].apply(check_fear_emojis)\n",
        "\n",
        "# Feature 7 - Emojis(Surprise)\n",
        "X_f7 = X_train['comments'].apply(check_surprise_emojis)\n",
        "\n",
        "# Feature 8 - Vowel Count\n",
        "X_f8 = X_train['comments'].apply(vowel_count)\n",
        "\n",
        "# Feature 9 - Consonant Count\n",
        "X_f9 = X_train['comments'].apply(consonant_count)\n",
        "\n",
        "# Feature 10 - Consonant Cluster\n",
        "X_f10 = X_train['comments'].apply(get_consonant_cluster)\n",
        "\n",
        "# Concatenate all features\n",
        "collected_features_train = pd.concat([X_f1, X_f2, X_f3, X_f4, X_f5, X_f6, X_f7, X_f8, X_f9, X_f10], axis=1)\n",
        "collected_features_train = collected_features_train.to_numpy();"
      ],
      "metadata": {
        "id": "7WCV4FBpv6YU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Extraction of Validation Set"
      ],
      "metadata": {
        "id": "Enu-LvNQ2Jzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature 1 - Word Frequency\n",
        "X_f1 = X_val['comments'].apply(wordFrequency)\n",
        "X_f1 = vectorizer.transform(X_val['comments'])\n",
        "X_f1 = pd.DataFrame(X_f1.toarray())\n",
        "\n",
        "# Feature 2 - Emojis(Sad)\n",
        "X_f2 = X_val['comments'].apply(check_sad_emojis)\n",
        "\n",
        "# Feature 3 - Emojis(Angry)\n",
        "X_f3 = X_val['comments'].apply(check_angry_emojis)\n",
        "\n",
        "# Feature 4 - Emojis(Joy)\n",
        "X_f4 = X_val['comments'].apply(check_joy_emojis)\n",
        "\n",
        "# Feature 5 - Emojis(Disgust)\n",
        "X_f5 = X_val['comments'].apply(check_disgust_emojis)\n",
        "\n",
        "# Feature 6 - Emojis(Fear)\n",
        "X_f6 = X_val['comments'].apply(check_fear_emojis)\n",
        "\n",
        "# Feature 7 - Emojis(Surprise)\n",
        "X_f7 = X_val['comments'].apply(check_surprise_emojis)\n",
        "\n",
        "# Feature 8 - Vowel Count\n",
        "X_f8 = X_val['comments'].apply(vowel_count)\n",
        "\n",
        "# Feature 9 - Consonant Count\n",
        "X_f9 = X_val['comments'].apply(consonant_count)\n",
        "\n",
        "# Feature 10 - Consonant Cluster\n",
        "X_f10 = X_val['comments'].apply(get_consonant_cluster)\n",
        "\n",
        "# Concatenate all features\n",
        "collected_features_val = pd.concat([X_f1, X_f2, X_f3, X_f4, X_f5, X_f6, X_f7, X_f8, X_f9, X_f10], axis=1)\n",
        "collected_features_val = collected_features_val.to_numpy();"
      ],
      "metadata": {
        "id": "crJ0rxwTwE2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert to Array"
      ],
      "metadata": {
        "id": "BQSqH2gKMfC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = y_train.to_numpy();\n",
        "y_train = np.squeeze(y_train)"
      ],
      "metadata": {
        "id": "8hR4JYiKLe02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "K Nearest Neighbor (Validation data)"
      ],
      "metadata": {
        "id": "ibcZ08bq2Lid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn_clf = KNeighborsClassifier(n_neighbors = 5)"
      ],
      "metadata": {
        "id": "0pZpxbL3Rgea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit model and predict using KNN"
      ],
      "metadata": {
        "id": "iwRb8_Xt4XVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# K Nearest Neighbor\n",
        "knn_clf.fit(collected_features_train,y_train)\n",
        "\n",
        "y_pred = knn_clf.predict(collected_features_val)\n",
        "\n",
        "print(classification_report(y_val, y_pred))\n",
        "print(confusion_matrix(y_val, y_pred))"
      ],
      "metadata": {
        "id": "2eHZS52LwCIS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "561d7249-5312-4289-bb1c-91bca632cb34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.42      0.51      0.46       296\n",
            "     disgust       0.88      1.00      0.93       302\n",
            "        fear       0.74      1.00      0.85       308\n",
            "         joy       0.45      0.41      0.43       297\n",
            "        none       0.42      0.31      0.35       485\n",
            "     sadness       0.47      0.26      0.33       312\n",
            "    surprise       0.77      1.00      0.87       298\n",
            "\n",
            "    accuracy                           0.61      2298\n",
            "   macro avg       0.59      0.64      0.60      2298\n",
            "weighted avg       0.58      0.61      0.58      2298\n",
            "\n",
            "[[151   7  15  20  67  25  11]\n",
            " [  0 302   0   0   0   0   0]\n",
            " [  0   0 308   0   0   0   0]\n",
            " [ 44   7  29 122  59  25  11]\n",
            " [104  22  37  96 148  40  38]\n",
            " [ 58   7  25  35  79  80  28]\n",
            " [  0   0   0   0   0   0 298]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multinomial Naive Bayes (Validation data)"
      ],
      "metadata": {
        "id": "dKJYnrVk2SNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnb_clf = MultinomialNB(alpha=1.0)"
      ],
      "metadata": {
        "id": "gYsCTTVFRcY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit model and predict using multinomail naive bayes"
      ],
      "metadata": {
        "id": "o7v-xzCM4pum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multinomial Naive Bayes\n",
        "mnb_clf.fit(collected_features_train, y_train)\n",
        "\n",
        "y_pred = mnb_clf.predict(collected_features_val)\n",
        "\n",
        "print(classification_report(y_val, y_pred))\n",
        "print(confusion_matrix(y_val, y_pred))"
      ],
      "metadata": {
        "id": "Sr6Kj0tNwNZw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9670785-f750-4c6b-cfd8-ef5f3b3ed9a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.63      0.81      0.71       296\n",
            "     disgust       0.94      0.98      0.96       302\n",
            "        fear       0.91      0.94      0.93       308\n",
            "         joy       0.67      0.74      0.70       297\n",
            "        none       0.71      0.54      0.61       485\n",
            "     sadness       0.73      0.67      0.70       312\n",
            "    surprise       0.91      0.92      0.92       298\n",
            "\n",
            "    accuracy                           0.78      2298\n",
            "   macro avg       0.79      0.80      0.79      2298\n",
            "weighted avg       0.78      0.78      0.78      2298\n",
            "\n",
            "[[239   4   5   3  33  11   1]\n",
            " [  7 295   0   0   0   0   0]\n",
            " [  2   2 289   1   3  11   0]\n",
            " [ 11   3   3 221  37   9  13]\n",
            " [ 65   7  13  83 263  42  12]\n",
            " [ 51   2   3  14  33 208   1]\n",
            " [  2   1   3  10   3   4 275]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree (Validation data)"
      ],
      "metadata": {
        "id": "cH7RxRcRNC5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt_clf = DecisionTreeClassifier()"
      ],
      "metadata": {
        "id": "LMGBGiYERPAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit model and predct using decision tree"
      ],
      "metadata": {
        "id": "GUh01KnE40-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree\n",
        "dt_clf.fit(collected_features_train, y_train)\n",
        "\n",
        "y_pred = dt_clf.predict(collected_features_val)\n",
        "\n",
        "print(classification_report(y_val, y_pred))\n",
        "print(confusion_matrix(y_val, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IA6NfKGNHM1",
        "outputId": "aabfd1f1-8e6b-4884-a3f4-27551887a96a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.71      0.68      0.70       296\n",
            "     disgust       0.99      1.00      0.99       302\n",
            "        fear       0.94      1.00      0.97       308\n",
            "         joy       0.71      0.73      0.72       297\n",
            "        none       0.63      0.60      0.61       485\n",
            "     sadness       0.74      0.72      0.73       312\n",
            "    surprise       0.95      1.00      0.98       298\n",
            "\n",
            "    accuracy                           0.80      2298\n",
            "   macro avg       0.81      0.82      0.81      2298\n",
            "weighted avg       0.80      0.80      0.80      2298\n",
            "\n",
            "[[202   0   5   7  59  22   1]\n",
            " [  0 302   0   0   0   0   0]\n",
            " [  0   0 308   0   0   0   0]\n",
            " [  4   1   4 217  59   9   3]\n",
            " [ 58   3   9  66 290  49  10]\n",
            " [ 20   0   2  14  51 224   1]\n",
            " [  0   0   0   0   0   0 298]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predict emotion/label using test data"
      ],
      "metadata": {
        "id": "8bW0o316Pp_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature 1 - Word Frequency\n",
        "X_f1 = X_test['comments'].apply(wordFrequency)\n",
        "X_f1 = vectorizer.transform(X_test['comments'])\n",
        "X_f1 = pd.DataFrame(X_f1.toarray())\n",
        "\n",
        "# Feature 2 - Emojis(Sad)\n",
        "X_f2 = X_test['comments'].apply(check_sad_emojis)\n",
        "\n",
        "# Feature 3 - Emojis(Angry)\n",
        "X_f3 = X_test['comments'].apply(check_angry_emojis)\n",
        "\n",
        "# Feature 4 - Emojis(Joy)\n",
        "X_f4 = X_test['comments'].apply(check_joy_emojis)\n",
        "\n",
        "# Feature 5 - Emojis(Disgust)\n",
        "X_f5 = X_test['comments'].apply(check_disgust_emojis)\n",
        "\n",
        "# Feature 6 - Emojis(Fear)\n",
        "X_f6 = X_test['comments'].apply(check_fear_emojis)\n",
        "\n",
        "# Feature 7 - Emojis(Surprise)\n",
        "X_f7 = X_test['comments'].apply(check_surprise_emojis)\n",
        "\n",
        "# Feature 8 - Vowel Count\n",
        "X_f8 = X_test['comments'].apply(vowel_count)\n",
        "\n",
        "# Feature 9 - Consonant Count\n",
        "X_f9 = X_test['comments'].apply(consonant_count)\n",
        "\n",
        "# Feature 10 - Consonant Cluster\n",
        "X_f10 = X_test['comments'].apply(get_consonant_cluster)\n",
        "\n",
        "# Concatenate all features\n",
        "collected_features_test = pd.concat([X_f1, X_f2, X_f3, X_f4, X_f5, X_f6, X_f7, X_f8, X_f9, X_f10], axis=1)\n",
        "collected_features_test = collected_features_test.to_numpy();"
      ],
      "metadata": {
        "id": "aB5az0nGPwa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "K Nearest Neighbor (Test data)"
      ],
      "metadata": {
        "id": "ZZnh_BQ-P8nn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = knn_clf.predict(collected_features_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQUENlHUP3Lk",
        "outputId": "87bb398e-f74d-4591-c038-a78b95a2be2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.43      0.47      0.45       301\n",
            "     disgust       0.87      1.00      0.93       299\n",
            "        fear       0.74      1.00      0.85       273\n",
            "         joy       0.45      0.43      0.44       295\n",
            "        none       0.46      0.34      0.39       509\n",
            "     sadness       0.50      0.30      0.38       306\n",
            "    surprise       0.76      1.00      0.87       314\n",
            "\n",
            "    accuracy                           0.62      2297\n",
            "   macro avg       0.60      0.65      0.61      2297\n",
            "weighted avg       0.59      0.62      0.59      2297\n",
            "\n",
            "[[142   5  20  25  64  27  18]\n",
            " [  0 299   0   0   0   0   0]\n",
            " [  0   0 273   0   0   0   0]\n",
            " [ 40  11  14 126  67  14  23]\n",
            " [ 96  23  40  93 171  51  35]\n",
            " [ 53   7  23  36  72  93  22]\n",
            " [  0   0   0   0   0   0 314]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multinomial Naive Bayes  (Test data)"
      ],
      "metadata": {
        "id": "DaipKduAQAcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = mnb_clf.predict(collected_features_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJjCyHGdP5zV",
        "outputId": "52394b57-67ea-48fd-b299-f202359d951a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.65      0.82      0.72       301\n",
            "     disgust       0.91      0.98      0.94       299\n",
            "        fear       0.89      0.94      0.91       273\n",
            "         joy       0.70      0.71      0.71       295\n",
            "        none       0.74      0.59      0.65       509\n",
            "     sadness       0.71      0.64      0.67       306\n",
            "    surprise       0.91      0.94      0.93       314\n",
            "\n",
            "    accuracy                           0.78      2297\n",
            "   macro avg       0.79      0.80      0.79      2297\n",
            "weighted avg       0.78      0.78      0.78      2297\n",
            "\n",
            "[[247   4   6   2  29  11   2]\n",
            " [  5 294   0   0   0   0   0]\n",
            " [  1   0 256   1   2  13   0]\n",
            " [ 13   3   5 209  40  16   9]\n",
            " [ 70  15  14  60 299  38  13]\n",
            " [ 42   6   7  15  34 197   5]\n",
            " [  3   2   0  10   0   3 296]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree  (Test data)"
      ],
      "metadata": {
        "id": "oIGlnawIQDQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Trees\n",
        "y_pred = dt_clf.predict(collected_features_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pj5DxxaLP7Ey",
        "outputId": "3cecc754-4203-4669-9fa9-8fcadcc6a688"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.68      0.71      0.69       301\n",
            "     disgust       0.99      1.00      0.99       299\n",
            "        fear       0.94      1.00      0.97       273\n",
            "         joy       0.71      0.75      0.73       295\n",
            "        none       0.67      0.61      0.64       509\n",
            "     sadness       0.74      0.67      0.70       306\n",
            "    surprise       0.94      1.00      0.97       314\n",
            "\n",
            "    accuracy                           0.80      2297\n",
            "   macro avg       0.81      0.82      0.81      2297\n",
            "weighted avg       0.80      0.80      0.80      2297\n",
            "\n",
            "[[213   1   2  11  58  16   0]\n",
            " [  0 299   0   0   0   0   0]\n",
            " [  0   0 273   0   0   0   0]\n",
            " [ 12   0   3 221  47   8   4]\n",
            " [ 60   3   8  65 311  49  13]\n",
            " [ 27   0   5  14  51 206   3]\n",
            " [  0   0   0   0   0   0 314]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qdoXbC-uPzmJ"
      }
    }
  ]
}